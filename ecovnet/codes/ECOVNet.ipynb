{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECOVNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmFdo9_WGhyQ"
      },
      "source": [
        "# Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND3Zyq42gCVF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chh46b6zcNcY"
      },
      "source": [
        "# Download and unzip the dataset into Colab's runtime environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzSrz-LLctTA"
      },
      "source": [
        "!cp \"/content/drive/My Drive/dataset.zip\" \"/content\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQeGP9qYkSaN"
      },
      "source": [
        "!mkdir \"/content/drive/My Drive/Colab Notebooks/Snapshot Ensemble\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzFhvD_1eV1q"
      },
      "source": [
        "!zip -FF ds.zip --out ds2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B9C8voOeYpR"
      },
      "source": [
        "!unzip -o -q \"ds2.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afN2s0chGpAU"
      },
      "source": [
        "# Import Tensorflow and Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoCqZY7zZZ1N"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-B4lMHfGvI2"
      },
      "source": [
        "# Data Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBQw6_PxUla_"
      },
      "source": [
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "base_dir = '/content/dataset'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = \"/content/dataset/test\" # Imbalanced test\n",
        "test_dir_100 = \"/content/drive/My Drive/Colab Notebooks/dataset/test_100\" # Balanced test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_b9rmJKV31S"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2521ztyqWgjZ"
      },
      "source": [
        "image_size = (240, 240) # efficienetB1\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "LEARNING_RATE=0.0001\n",
        "num_classes=3\n",
        "epochs = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCtY_M3OGzIK"
      },
      "source": [
        "# Training, Validation and Testing data load and Train Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o9JIFJ03YVO"
      },
      "source": [
        "trdata=tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=90,\n",
        "    shear_range=0.10,\n",
        "    zoom_range=0.10,    \n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "train_ds = trdata.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=image_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\",\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "valdata=tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "val_ds = trdata.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=image_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\",\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "testdata=tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "test_ds = testdata.flow_from_directory(\n",
        "    directory=test_dir,\n",
        "    target_size=image_size,\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "testdata_100=tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "test_ds_100 = testdata_100.flow_from_directory(\n",
        "    directory=test_dir_100,\n",
        "    target_size=image_size,\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypctLqE7_Wux"
      },
      "source": [
        "# Training, Validation and Testing data load and Train without Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3AtiE0y_kJr"
      },
      "source": [
        "trdata=tf.keras.preprocessing.image.ImageDataGenerator( \n",
        "    validation_split=0.1   \n",
        ")\n",
        "\n",
        "train_ds = trdata.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=image_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\",\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "valdata=tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "val_ds = trdata.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=image_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\",\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "testdata=tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "test_ds = testdata.flow_from_directory(\n",
        "    directory=test_dir,\n",
        "    target_size=image_size,\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "testdata_100=tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "test_ds_100 = testdata_100.flow_from_directory(\n",
        "    directory=test_dir_100,\n",
        "    target_size=image_size,\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwsHxmaG24O1"
      },
      "source": [
        "# Class Weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WrxYwpd29wT"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "               'balanced',\n",
        "                np.unique(train_ds.classes), \n",
        "                train_ds.classes)\n",
        "\n",
        "print(class_weights)\n",
        "print(train_ds.class_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBIHU9DbATK-"
      },
      "source": [
        "# Efficientnet Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvWTrBUkLZde"
      },
      "source": [
        "!pip install -U git+https://github.com/qubvel/efficientnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh1pFNuU3CYb"
      },
      "source": [
        "# Swish Activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y30GDcMa3Ja0"
      },
      "source": [
        "from keras.backend import sigmoid\n",
        "from keras.utils import get_custom_objects\n",
        "from keras.layers import Activation\n",
        "\n",
        "class SwishActivation(Activation):\n",
        "    \n",
        "    def __init__(self, activation, **kwargs):\n",
        "        super(SwishActivation, self).__init__(activation, **kwargs)\n",
        "        self.__name__ = 'swish_act'\n",
        "\n",
        "def swish_act(x, beta = 1):\n",
        "    return (x * sigmoid(beta * x))\n",
        "\n",
        "\n",
        "get_custom_objects().update({'swish_act': SwishActivation(swish_act)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz-t1xGcBIDA"
      },
      "source": [
        "# Cosine Annealing Snapshot Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__YDjHYiBCXL"
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "from keras.optimizers import SGD\n",
        "from keras import backend\n",
        "from math import pi\n",
        "from math import cos\n",
        "from math import floor\n",
        " \n",
        "# snapshot ensemble with custom learning rate schedule\n",
        "\n",
        "class SnapshotEnsemble(Callback):\n",
        "\t# constructor\n",
        "\tdef __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):\n",
        "\t\tself.epochs = n_epochs\n",
        "\t\tself.cycles = n_cycles\n",
        "\t\tself.lr_max = lrate_max\n",
        "\t\tself.lrates = list()\n",
        "\n",
        "\t# calculate learning rate for epoch\n",
        "\tdef cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n",
        "\t\tepochs_per_cycle = floor(n_epochs/n_cycles)\n",
        "\t\tcos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n",
        "\t\treturn lrate_max/2 * (cos(cos_inner) + 1)\n",
        "\n",
        "\t# calculate and set learning rate at the start of the epoch\n",
        "\tdef on_epoch_begin(self, epoch, logs={}):\n",
        "\t\t# calculate learning rate\n",
        "\t\tlr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)\n",
        "\t\t# set learning rate\n",
        "\t\tbackend.set_value(self.model.optimizer.lr, lr)\n",
        "\t\t# log value\n",
        "\t\tself.lrates.append(lr)\n",
        "\n",
        "\t# save models at the end of each cycle\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\t# check if we can save model\n",
        "\t\tepochs_per_cycle = floor(self.epochs / self.cycles)\n",
        "\t\n",
        "\t\tif epoch != 0 and (epoch + 1) % epochs_per_cycle == 0:\n",
        "\t\t\t# save model to file\n",
        "\t\t\t#filename = \"snapshot_model_%d.h5\" % int((epoch + 1) / epochs_per_cycle)\n",
        "\t\t\t\n",
        "\t\t\tfilename = \"/content/drive/My Drive/Colab Notebooks/Snapshot Ensemble/snapshot_model_%d.h5\" % int((epoch + 1) / epochs_per_cycle)\n",
        "\t\n",
        "\t\t\tself.model.save(filename)\n",
        "\t\t\tprint('>saved snapshot %s, epoch %d' % (filename, epoch))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AqNUP9DP70M"
      },
      "source": [
        "# Modified Model Build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFlNwWjW5USU"
      },
      "source": [
        "from keras import layers\n",
        "from keras.layers import AveragePooling2D, BatchNormalization\n",
        "from keras.layers import GlobalAveragePooling2D,ZeroPadding2D\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "from keras.applications import DenseNet121\n",
        "from keras.applications import MobileNetV2\n",
        "from keras.regularizers import l2\n",
        "from keras.regularizers import l1_l2\n",
        "\n",
        "from keras import optimizers\n",
        "import efficientnet.tfkeras as enet\n",
        "\n",
        "\n",
        "base_model = enet.EfficientNetB1(include_top=False, input_shape=(240,240,3), pooling='avg', weights=\"imagenet\",classes=num_classes)\n",
        "\n",
        "x = base_model.output\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Dense(512,kernel_regularizer=l1_l2(l1=1e-5, l2=1e-3))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(swish_act)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "x = Dense(512,kernel_regularizer=l1_l2(l1=1e-5, l2=1e-3))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(swish_act)(x) \n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# Output layer\n",
        "predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model= Model(inputs = base_model.input, outputs = predictions)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=LEARNING_RATE),metrics=['acc'])\n",
        "model.summary()\n",
        "model.save('/content/drive/My Drive/Colab Notebooks/Snapshot Ensemble/model_base.h5') # Model save for ensemble"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE-P76jlHBzA"
      },
      "source": [
        "# Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc1zvKfLGJb3"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
        "from keras.callbacks import CSVLogger\n",
        "csv_logger = CSVLogger('training.log', separator=\",\", append=True)\n",
        "\n",
        "checkpoint = ModelCheckpoint('/content/drive/My Drive/Colab Notebooks/Snapshot Ensemble/model_weight.h5', monitor='val_acc',save_best_only=True,)\n",
        "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, verbose=1,)\n",
        "\n",
        "n_cycles = epochs / 6\n",
        "ca = SnapshotEnsemble(epochs, n_cycles, 0.0001)\n",
        "\n",
        "csv_logger = CSVLogger(\"/content/drive/My Drive/Colab Notebooks/Snapshot Ensemble/training.log\", separator=\",\", append=True)\n",
        "\n",
        "history=model.fit(\n",
        "    train_ds, epochs=epochs, callbacks=[ca,checkpoint,csv_logger], validation_data=val_ds\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}